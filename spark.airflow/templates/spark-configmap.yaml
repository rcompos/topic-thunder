apiVersion: v1
data:
  spark-defaults.conf: |
    #
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    #
    # Default system properties included when running spark-submit.
    # This is useful for setting default environmental settings.
    # Example:
    # spark.master                     spark://master:7077
    # spark.eventLog.enabled           true
    # spark.eventLog.dir               hdfs://namenode:8021/directory
    # spark.serializer                 org.apache.spark.serializer.KryoSerializer
    # spark.driver.memory              5g
    # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
    spark.master                                    spark://spark-master-svc.spark:7077
    spark.scheduler.mode                            FAIR
    spark.driver.cores                              2
    # spark.executor.cores                            2
    spark.executor.cores                            1
    # spark.executor.memory                           20G
    spark.executor.memory                           1G
    # spark.driver.memory                             4G
    # spark.driver.maxResultSize                      2G
    spark.driver.memory                             2G
    spark.driver.maxResultSize                      1G
    #spark.hadoop.fs.s3a.impl                        org.apache.hadoop.fs.s3a.S3AFileSystem
    #spark.sql.catalogImplementation                 hive
    spark.network.timeout                           600s
    #spark.hadoop.hive.metastore.uris                thrift://hive.spark:9083
    #spark.hadoop.hive.metastore.client.socket.timeout 1800s
    spark.dynamicAllocation.minExecutors            1
    # spark.dynamicAllocation.maxExecutors            40
    spark.dynamicAllocation.maxExecutors            5
    spark.dynamicAllocation.initialExecutors        1
    spark.dynamicAllocation.enabled                 true
    spark.shuffle.service.enabled                   false
    spark.dynamicAllocation.shuffleTracking.enabled true
    spark.dynamicAllocation.executorIdleTimeout     30s
    spark.sql.execution.arrow.pyspark.enabled       True
    spark.serializer                                org.apache.spark.serializer.KryoSerializer
    #spark.sql.hive.metastorePartitionPruning        True
    spark.sql.orc.filterPushdown                    True
    spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2
    spark.sql.shuffle.partitions                    200
    spark.sql.extensions                            org.apache.spark.sql.hudi.HoodieSparkSessionExtension,io.delta.sql.DeltaSparkSessionExtension
    spark.sql.catalog.spark_catalog                 org.apache.spark.sql.delta.catalog.DeltaCatalog
    spark.hadoop.hoodie.metadata.enable             true
    spark.sql.files.ignoreCorruptFiles              true 
    #spark.hadoop.fs.s3a.experimental.input.fadvise  random
    #spark.hadoop.fs.s3a.readahead.range             157810688
    spark.kryoserializer.buffer.max                 128m
    spark.network.timeout                           60s
    spark.sql.sources.bucketing.enabled             False
    spark.sql.mapKeyDedupPolicy                     LAST_WIN
    #spark.eventLog.enabled                          True
    #spark.eventLog.dir                              s3a://{{ .Values.s3BucketPath }}/history
    #spark.history.fs.logDirectory                   s3a://{{ .Values.s3BucketPath }}/history

  
kind: ConfigMap
metadata:
  name: spark-conf
  namespace: spark
  metadata:
  labels:
    app: spark
    argocd.argoproj.io/instance: jupyterhub
    chart: bitnami-spark-1.x.x
    component: hub
    heritage: Helm
    release: 5.4.22
